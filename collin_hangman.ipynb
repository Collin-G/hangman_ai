{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def read_book(file_path):\n",
    "    \"\"\"Read the content of the book from the file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def write_unique_words(words, output_path):\n",
    "    \"\"\"Write each unique word to the output file, one word per line.\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        for word in words:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Process the text to normalize and extract unique words.\"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    # Get unique words\n",
    "    unique_words = set(words)\n",
    "    return unique_words\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    \"\"\"Main function to read, process, and write the text.\"\"\"\n",
    "    # Read the book\n",
    "    text = read_book(input_file)\n",
    "    # Process the text to get unique words\n",
    "    unique_words = process_text(text)\n",
    "    # Write unique words to the output file\n",
    "    write_unique_words(unique_words, output_file)\n",
    "\n",
    "input_file = './book.txt'\n",
    "output_file = './words.txt'\n",
    "main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOKENIZE_LAMBDA = lambda x : list(x)\n",
    "VOCAB = {c : i+2 for i, c in enumerate(list('abcdefghijklmnopqrstuvwxyz'))}\n",
    "VOCAB['_'] = 1 # mask\n",
    "VOCAB['$'] = 0 # pad\n",
    "VOCAB['%'] = len(VOCAB) # sep\n",
    "PAD_IDX = VOCAB['$']\n",
    "\n",
    "VOCAB_TRANSFORM_LAMBDA = lambda toks : torch.tensor(np.vectorize(lambda x : VOCAB[x])(toks) )\n",
    "TOKENIZE_LAMBDA = lambda x : list(x)\n",
    "TEXT_TRANSFORM_LAMBDA = lambda x : VOCAB_TRANSFORM_LAMBDA(TOKENIZE_LAMBDA(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FILENAME = output_file\n",
    "import re\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import math\n",
    "class DataTools:\n",
    "\n",
    "  @staticmethod\n",
    "  def mask_characters(word, num_chars_to_mask=-1, to_mask=False):\n",
    "      if type(word) == float:\n",
    "          print(word)\n",
    "          word = str(word)\n",
    "      if not to_mask:\n",
    "          to_mask = rand.random()*0.9\n",
    "      if num_chars_to_mask == -1:\n",
    "          num_chars_to_mask = int(max([1, int(to_mask*len(set(word)))]))\n",
    "\n",
    "      # Ensure num_chars_to_mask is within the valid range\n",
    "      if num_chars_to_mask < 1 or num_chars_to_mask > len(set(word)):\n",
    "          raise ValueError(\"Number of characters to mask is out of range\")\n",
    "\n",
    "      # Select unique characters from the word\n",
    "      unique_chars = list(set(word))\n",
    "\n",
    "      # Randomly choose num_chars_to_mask characters to mask\n",
    "      chars_to_mask = rand.sample(unique_chars, num_chars_to_mask)\n",
    "\n",
    "      # Create the masked word\n",
    "      masked_word = ''.join(['_' if char in chars_to_mask else char for char in word])\n",
    "\n",
    "      cannot_guess = sorted(list(set(\"abcdefghijklmnop\") - (set(word))))\n",
    "\n",
    "      random_guessed_letters = rand.sample(cannot_guess, min(6, int(rand.random()*len(cannot_guess))+1))\n",
    "\n",
    "      label = DataTools.get_label(word, masked_word, to_mask)\n",
    "      for c in random_guessed_letters:\n",
    "          label[VOCAB[c]] = 0#-1\n",
    "\n",
    "      return masked_word + \"%\" + ''.join(sorted(random_guessed_letters)), label\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def get_dictionary_df(txt_filename=TXT_FILENAME):\n",
    "      return pd.read_csv(filepath_or_buffer=txt_filename, encoding=\"utf8\", names=['words'])\n",
    "  \n",
    "  @staticmethod\n",
    "  def get_label(word, masked, to_mask=False):\n",
    "      guessable = set(word) - (set(masked))\n",
    "      freqs = [0]*len(VOCAB)\n",
    "\n",
    "      for c in word:\n",
    "          freqs[VOCAB[c]] += 1 + (freqs[VOCAB[c]]) if c in guessable else 0  \n",
    "\n",
    "      for c in range(len(freqs)):\n",
    "          if freqs[c] == 0:\n",
    "              freqs[c] = -torch.inf\n",
    "\n",
    "      return torch.softmax(torch.tensor(freqs), dim=0)*math.sqrt(1-to_mask)*2\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def get_dataset_from_df(df):\n",
    "      df['transformed'] = df['words'].apply(DataTools.mask_characters)\n",
    "      df['src'] = df['transformed'].apply(lambda x : x[0])\n",
    "      df['tgt'] = df['transformed'].apply(lambda x : x[1])\n",
    "\n",
    "      src, tgt = list(df['src']), list(df['tgt'])\n",
    "      return src, tgt\n",
    "\n",
    "\n",
    "class DictionaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt_filename=TXT_FILENAME, truncate=False) -> None:\n",
    "        # load data + preprocess\n",
    "        df = DataTools.get_dictionary_df(txt_filename)[:truncate] if truncate else DataTools.get_dictionary_df(txt_filename)\n",
    "        self.src, self.tgt = DataTools.get_dataset_from_df(df)\n",
    "        self.src = list(self.src)\n",
    "        self.tgt = self.tgt\n",
    "\n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        return self.src[idx], self.tgt[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer, TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class MaskedLmTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(MaskedLmTransformer, self).__init__()\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layer=self.encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "        self.src_tok_emb = TokenEmbedding(vocab_size, emb_size)\n",
    "        self.generator = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, word):\n",
    "        embedded = self.src_tok_emb(word)\n",
    "        emb_1 = self.positional_encoding(embedded)\n",
    "\n",
    "        encoded = self.encoder(emb_1)\n",
    "        encoded = encoded.mean(1)\n",
    "\n",
    "        return self.generator(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  @staticmethod\n",
    "  def get_train_test_iter():\n",
    "    total_data_iter = DictionaryDataset()#truncate=100)\n",
    "    train_size = int(0.8 * len(total_data_iter))\n",
    "    test_size = len(total_data_iter) - train_size\n",
    "    train_iter, test_iter = torch.utils.data.random_split(total_data_iter, [train_size, test_size])\n",
    "    return train_iter, test_iter\n",
    "\n",
    "  @staticmethod\n",
    "  def train_epoch(model, optimizer, loss_fn,train_iter, batch_size):\n",
    "      model.train()\n",
    "      losses = 0\n",
    "      train_dataloader = DataLoader(train_iter, batch_size=batch_size, collate_fn=Trainer.collate_fn)\n",
    "\n",
    "      for src, tgt in train_dataloader:\n",
    "          src = src.to(DEVICE)\n",
    "          tgt = tgt.to(DEVICE)#, dtype=torch.float)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          logits = model(src)\n",
    "\n",
    "          loss = loss_fn(logits, tgt)\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "          losses += loss.item()\n",
    "\n",
    "      return losses/len(list(train_dataloader))\n",
    "\n",
    "  @staticmethod\n",
    "  def evaluate(model, test_iter, loss_fn,batch_size):\n",
    "      model.eval()\n",
    "      losses = 0\n",
    "\n",
    "      val_dataloader = DataLoader(test_iter, batch_size=batch_size, collate_fn=Trainer.collate_fn)\n",
    "\n",
    "\n",
    "      for src, tgt in val_dataloader:\n",
    "          src = src.to(DEVICE)\n",
    "          tgt = tgt.to(DEVICE)#, dtype=torch.float)\n",
    "\n",
    "          logits = model(src)\n",
    "\n",
    "          loss = loss_fn(logits, tgt)\n",
    "          losses += loss.item()\n",
    "\n",
    "      return losses / len(list(val_dataloader))\n",
    "  \n",
    "  @staticmethod\n",
    "  def train(model, optimizer, loss_fn,num_epochs, batch_size):\n",
    "    train_iter, test_iter = Trainer.get_train_test_iter()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        start_time = timer()\n",
    "        train_loss = Trainer.train_epoch(model, optimizer, loss_fn, train_iter, batch_size)\n",
    "        end_time = timer()\n",
    "        val_loss = Trainer.evaluate(model, test_iter, loss_fn, batch_size)\n",
    "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "  \n",
    "  @staticmethod\n",
    "  def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(TEXT_TRANSFORM_LAMBDA(src_sample))\n",
    "        tgt_batch.append((list(tgt_sample)))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, torch.tensor(tgt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(VOCAB)\n",
    "EMB_SIZE = 26\n",
    "NHEAD = 2\n",
    "FFN_HID_DIM = 26\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 10\n",
    "\n",
    "transformer = MaskedLmTransformer(\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    emb_size=EMB_SIZE,\n",
    "    nhead=NHEAD,\n",
    "    vocab_size=SRC_VOCAB_SIZE,\n",
    "    dim_feedforward=FFN_HID_DIM,\n",
    "    dropout= 0.1\n",
    ")\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model was already trained over Google Colab\n",
    "\n",
    "# Uncomment this code if you wish to train the model again \n",
    "# Trainer.train(transformer, optimizer, loss_fn, NUM_EPOCHS, BATCH_SIZE)\n",
    "\n",
    "# Uncomment this code to save the model into a file after training\n",
    "# torch.save(transformer, f=\"/content/drive/My Drive/saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_liks(word, transformer):\n",
    "    prediction = transformer(torch.tensor([list(TEXT_TRANSFORM_LAMBDA(word))]))\n",
    "    return prediction[0]\n",
    "\n",
    "def guess_letter(masked, excluded, transformer):\n",
    "    print(masked)\n",
    "    liks = get_liks(masked, transformer=transformer)\n",
    "    reverse_voca_lookup = {VOCAB[k]:k for k in VOCAB}\n",
    "    pairs = {reverse_voca_lookup[i]:liks[i] for i in range(len(liks))}\n",
    "    for c in excluded:\n",
    "        pairs.pop(c)\n",
    "    pairs = dict(sorted(pairs.items(), key=lambda item: item[1], reverse=True))\n",
    "    lets = [p[0] for p in pairs]\n",
    "    return lets[0], lets, pairs\n",
    "\n",
    "\n",
    "def play_game(model, word):\n",
    "    if type(model) == str:\n",
    "        model = torch.load(model)\n",
    "    masked = '_'*len(word)\n",
    "\n",
    "    wrongs = 0\n",
    "    guessed = set()\n",
    "    wrong_guesses = set()\n",
    "\n",
    "    while(wrongs < 6):\n",
    "        in_word = set(masked) - {'_'}\n",
    "        guess, _, _ = guess_letter(masked, excluded=guessed.union(in_word), transformer=model)\n",
    "        masked = ''.join([word[i] if word[i] == guess else masked[i] for i in range(len(word))]) + \"%\" + ''.join(sorted(list(wrong_guesses)))\n",
    "        print(guess, masked)\n",
    "\n",
    "        guessed.add(guess)\n",
    "        if guess not in set(word):\n",
    "            wrongs+=1\n",
    "            wrong_guesses.add(guess)\n",
    "\n",
    "        if masked[0:masked.find(\"%\")]==word:\n",
    "            print(\"YOU WON!\")\n",
    "            print(wrongs, \" wrong guesses\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "e ___e______%\n",
      "___e______%\n",
      "i ___e___i__%\n",
      "___e___i__%\n",
      "o ___e___io_%\n",
      "___e___io_%\n",
      "h ___e___io_%\n",
      "___e___io_%\n",
      "a _a_e_a_io_%h\n",
      "_a_e_a_io_%h\n",
      "r _a_era_io_%h\n",
      "_a_era_io_%h\n",
      "l la_era_io_%h\n",
      "la_era_io_%h\n",
      "s la_era_io_%h\n",
      "la_era_io_%h\n",
      "m la_era_io_%hs\n",
      "la_era_io_%hs\n",
      "t la_eratio_%hms\n",
      "la_eratio_%hms\n",
      "n la_eration%hms\n",
      "la_eration%hms\n",
      "p la_eration%hms\n",
      "la_eration%hms\n",
      "f la_eration%hmps\n",
      "la_eration%hmps\n",
      "c laceration%fhmps\n",
      "YOU WON!\n",
      "5  wrong guesses\n"
     ]
    }
   ],
   "source": [
    "play_game(\"rose\", \"laceration\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
